{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 0 (Know your Data) - Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.1. Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()\n",
    "import math\n",
    "import statistics\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn import neighbors,tree, preprocessing\n",
    "from sklearn.metrics import confusion_matrix, precision_recall_fscore_support, classification_report, recall_score, accuracy_score\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB, CategoricalNB, BernoulliNB, ComplementNB\n",
    "from sklearn.ensemble import RandomForestClassifier \n",
    "import re\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression, Perceptron, RidgeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('PetFinder_dataset.csv')\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading colour, breed and state labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_labels = pd.read_csv('color_labels.csv').set_index('ColorID')\n",
    "breed_labels = pd.read_csv('breed_labels.csv').set_index('BreedID')\n",
    "state_labels = pd.read_csv('state_labels.csv').set_index('StateID')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.2. Understanding Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How big is the dataset?\n",
    "The dataset contains 14993 entries with 24 features each.\n",
    "\n",
    "#### Is this the entire dataset?\n",
    "The original dataset from PetFinder.my contains over 150,000 animal profiles.\n",
    "\n",
    "#### Is this data representative enough?\n",
    "The dataset represents ~10% of the original data. EDA is required to best assess whether it can be considered a representative sample of the original population.\n",
    "\n",
    "#### Are there likely to be gross outliers or extraordinary sources of noise?\n",
    "EDA can shed light on this topic, by representing data distribution and irregularities that could point to errors in data.\n",
    "\n",
    "#### Are there any fields that are unique identifiers? These are the fields you might use for joining between datasets, etc.\n",
    "All animal profiles are uniquely identified by a PetID. Each PetID also has a non-unique RescuerID, and a non-unique State.\n",
    "\n",
    "#### When data entries are blank, where does that come from?\n",
    "Some animal profiles have no assigned name. There are also some features which have a default value set for cases in which that feature is not specified or applicable to the data entry.\n",
    "\n",
    "#### How common are blank entries?\n",
    "There are ~1200 blank name entries, which constitute a significant portion of the data. It would perhaps be best to keep this data in the dataset for analysis. Other features with default value entries should also be kept, because, like a profile without a name, unspecified animal characteristics have a meaning relevant to our objectives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to better understand our dataset and to make decisions about feature selection, feature extraction and general cleaning of the data, we started by plotting distribution of the original features and the target."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Target: *AdoptionSpeed*\n",
    "\n",
    "The target feature displayed a clear discrepancy in distribution, showing a significantly lower number of examples for `AdoptionSpeed = 0`. This imbalance should be addressed, as models tend to neglect minority classes if they don't have a large enough representation in the overall dataset, therefore compromising the classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.catplot(x=\"AdoptionSpeed\",data=df, kind='count')\n",
    "(ax.set_axis_labels(\"Adoption Speed\", \"Number of Pets\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overall correlations\n",
    "The plot bellow illustrates the absolute correlation between  each feature and the target class `AdoptionSpeed`. All correlations are very low (<15%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_df = df.select_dtypes(exclude=['object'])\n",
    "corr_dict = {}\n",
    "for feature in numeric_df.columns:\n",
    "    corr_dict[feature] = abs(df['AdoptionSpeed'].corr(df[feature]))\n",
    "    #print(feature, '-->', corr_dict[feature])\n",
    "    \n",
    "corr_dict.pop('AdoptionSpeed')\n",
    "plt.figure(figsize=(20,7))\n",
    "plt.bar(range(len(corr_dict)), list(corr_dict.values()), align='center')\n",
    "plt.xticks(range(len(corr_dict)), list(corr_dict.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PetID\n",
    "\n",
    "Since PetID is not informative of the pet profile (it acts solely as a unique identifier of the page), it would be excluded from the analysis, although it will be used as an index for the dataFrame. It can be later useful when evaluating model performance, as the respective record values can reveal if certain features were appropriately categorised."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.set_index(\"PetID\", inplace=True) # change index to PetId\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Strategy\n",
    "\n",
    "The following analysis was done to each feature:\n",
    "* Check the number of examples for each feature values.\n",
    "* Check the distribution of each feature values' among AdoptionSpeed classes.\n",
    "\n",
    "For the second step we used the following visualisation method, which returns not only a plot on the distribution but also a table of representativity percentage gain of each feature's values for each `AdoptionSpeed` class, relative to `AdoptionSpeed = 0`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creates a stacked bar plot for any two categorical variables\n",
    "# also creates a relative frequency version of the same plot\n",
    "# and a relative gain compared to AdoptionSpeed=0 \n",
    "# frel=False to return only the absolute frequency plot\n",
    "\n",
    "def cat_distr_pairwise(cat_x,cat_y, df, frel):\n",
    "    \n",
    "    unique_cat_y = sorted(df[cat_y].unique()) #ordered list of categories in variable y\n",
    "    unique_cat_x = sorted(df[cat_x].unique()) #ordered list of categories in variable x\n",
    "    cat_count = pd.DataFrame(columns=unique_cat_y,index=unique_cat_x) #DataFrame of x categories by y categories\n",
    "    for y in unique_cat_y: #count each type of y category...\n",
    "        for x in unique_cat_x: #... for each type of x category (e.g.: all dogs' adoption speed, then all cats' adoption speed)\n",
    "            count = df[cat_y][(df[cat_x]==x) & (df[cat_y]==y)].count() \n",
    "            cat_count.at[x,y] = count \n",
    "    \n",
    "    def color(val):\n",
    "        if val < 0:\n",
    "            color = 'red'\n",
    "        elif val>0:\n",
    "            color = 'green'\n",
    "        else:\n",
    "            color='white'\n",
    "        return 'background-color: %s' % color\n",
    "    \n",
    "    if frel==False:\n",
    "        \n",
    "        #relative gain table\n",
    "        relgain = cat_count.pct_change().cumsum() * 100\n",
    "        print(\"Relative gain from AdoptionSpeed=0 (%):\")\n",
    "        display(relgain.style.applymap(color))\n",
    "        # alternative: display(relgain.style.bar(align='zero', color=['#d65f5f', '#5fba7d'])\n",
    "        return cat_count.plot.bar(stacked=True, figsize=(10,7))\n",
    "    \n",
    "    else:\n",
    "        #relative frequency stacked bar plot\n",
    "        freq_cat_count = cat_count.divide(cat_count.sum(axis=1), axis=0)\n",
    "        \n",
    "        #relative gain table\n",
    "        relgain = freq_cat_count.pct_change().cumsum() * 100\n",
    "        print(\"Relative gain from AdoptionSpeed=0 (%):\")\n",
    "        display(relgain.style.applymap(color))\n",
    "        # alternative: display(relgain.style.bar(align='zero', color=['#d65f5f', '#5fba7d'])\n",
    "\n",
    "        return freq_cat_count.plot.bar(stacked=True, figsize=(10,7), title=cat_y + \" relative distribution among AdoptionSpeed\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now present the most important conclusions taken from this analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Type\n",
    "\n",
    "Because we are going to later create type-specific predictive models, it would be relevant to see the number of cases we have in our dataset for each animal (cat or dog)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "bx = sns.catplot(x=\"Type\",data=df, kind='count')\n",
    "(bx.set_axis_labels(\"Type\", \"Number of Pets\")\n",
    "    .set_xticklabels([\"Dogs\", \"Cats\"])\n",
    "    .set_titles(\"{col_name} {col_var}\")\n",
    "    .despine(left=True))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_distr_pairwise('AdoptionSpeed','Type', df, frel=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at our data, the number of examples of dogs and cats is fairly proportional, as well as their relative distribution among target classes. \n",
    "Looking at the gain table, it is clear that dogs (1) tend to get adopted later relative to cats (2), which we can see by the growing relative gain. In fact, the number of dogs with no adoption after 100 days (`AdoptionSpeed = 4`) is 37% higher than dogs being adopted on the same day (`AdoptionSpeed = 0`). On the other hand, we observe the opposite phenomena on cats. They tend to be adopted at ealier stages, being the number of cats with no adoption after 100 days (`AdoptionSpeed = 4`) is 29% lower than cats being adopted on the same day (`AdoptionSpeed = 0`).\n",
    "Despite this small discrepancies, we expect the resulting models' performance to be comparable.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Name\n",
    "Since a reasonable amount of the profiles didn't make any reference to name (empty) a first approach would be to fill those empty values with a fixed value such as \"No Name\" or similar.\n",
    "Other approach would be to transform this feature into a binary one with a 0 value for profiles with no name and a 1 value for the opposite.\n",
    "Since it was later found that some of the named profiles were filled with \"No Name\" and derivatives of this name such as \"No Name Yet\", \"V6\", \"å°è±¹çº¹\" etc., which are not proper names, we made an attempt to classify those cases as \"No name\" as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new feature - hasNames\n",
    "l = []\n",
    "ignore = []\n",
    "\n",
    "for i in df[\"Name\"]:\n",
    "    if type(i)!=str: # Some empty descriptions are type:float\n",
    "        l.append(0)\n",
    "    else:\n",
    "        if len(i)<3: #Consider that a 2 letter name is not a proper name.\n",
    "            l.append(0)\n",
    "            ignore.append(i)\n",
    "        elif re.search(\"[a-zA-Z]\", i) == None: #If name doesn't have letters, it's not a proper name.\n",
    "            l.append(0)\n",
    "            ignore.append(i)\n",
    "        elif re.search(\"unnamed|no name\",i, re.IGNORECASE) != None: \n",
    "            #If name string includes \"unnamed\" or \"no name\", it's not a proper name\n",
    "            l.append(0)\n",
    "            ignore.append(i)\n",
    "        elif len(i.split())>0:\n",
    "            l.append(1)\n",
    "\n",
    "print(\"A glimpse on the ignored names:\",ignore[:50])\n",
    "df_processed1 = df.copy()\n",
    "df_processed1.drop(\"Name\", axis=1, inplace = True)\n",
    "df_processed1.insert(1,\"hasName\",l) ##INSERT IN DATAFRAME\n",
    "\n",
    "sns.set(style=\"whitegrid\")\n",
    "ax = sns.countplot(x=\"hasName\", data=df_processed1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_distr_pairwise('AdoptionSpeed','hasName', df_processed1, frel=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from the plot above, the relative distribution of the \"noName\" feature values among `AdoptionSpeed` classes is very similar. This probabably means that the pet's name is not a very important factor in the adopter's decision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Age\n",
    "\n",
    "When looking at `Age` distribution, we can see that there are peaks every 12 months. Young pets (less than 1 year old) are the most frequent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = df['Age'] # get Age array\n",
    "nr_bins = 1 + 3.322*math.log(len(a),2) # Number of bins according to Sturges rule\n",
    "sns.set(rc={'figure.figsize':(15,5)}) # set figure size\n",
    "\n",
    "# setting up the axis\n",
    "fig, ax = plt.subplots()\n",
    "ticks = [i for i in range(0,260,12)]\n",
    "ticks.pop(0)\n",
    "ax.set_xticks(ticks)\n",
    "plt.xlim([0.0,250])\n",
    "\n",
    "sns.distplot(a, bins=round(nr_bins), kde=False, axlabel=\"Age (months)\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The average cat is considered a kitten roughly until it reaches the age of 1 year old. The same goes for puppies. On the other hand, dogs and cats are considered Seniors roughly when they reach the age of 6 years. Of course, these are approximations, as the classification varies with the type of animal (cats or dogs), breed, size and many other factors.\n",
    "\n",
    "In this sense, we will consider these categories:\n",
    "* Baby - age 0-11 months => 0\n",
    "* Adult - age 12-71 months =>1\n",
    "* Senior - age 72-250 months =>2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new feature - AgeGroup\n",
    "AgeGroup = []\n",
    "for i in df[\"Age\"]:\n",
    "    if i<12:\n",
    "        AgeGroup.append(0)\n",
    "    elif i<72:\n",
    "        AgeGroup.append(1)\n",
    "    else:\n",
    "        AgeGroup.append(2)\n",
    "\n",
    "df_processed2 = df_processed1.copy()\n",
    "df_processed2.drop(\"Age\", axis=1, inplace = True)\n",
    "df_processed2.insert(2,\"AgeGroup\",AgeGroup) ##INSERT IN DATAFRAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_distr_pairwise('AdoptionSpeed','AgeGroup', df_processed2, frel=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The relative distribution of `AgeGroup` among `AdoptionSpeed` indicates that there seems to be no particular correlation between being younger and getting adopted earlier. Even without the dicretization we can observe the same phenomena: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,15))\n",
    "plt.ylim(0, 260)\n",
    "sns.boxplot(data = df, x='AdoptionSpeed', y='Age')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Breed1 and Breed2\n",
    "\n",
    "According to the breeds feature labels, dogs breeds go from 1 to 240, plus 307, and cats breeds go from 241 to 306. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bx1 = sns.catplot(x=\"Breed1\",data=df, kind='count',height=9, aspect=3)\n",
    "bx2 = sns.catplot(x=\"Breed2\",data=df, kind='count',height=9, aspect=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at our data, and according to the breeds feature labels, we found some cats with dogs breeds assigned to them (15, 21, 25, 70, 114, 205, 218, 307).\n",
    "Therefore, and given the distribution of pets by breeds in which some breeds are poorly representated perhaps a good idea should be grouping pets by pure-race/mixed-race."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating new feature: Pure breed\n",
    "l3 =[]\n",
    "for i in range(len(df)):\n",
    "    if df['Breed1'][i]==307 or df['Breed2'][i]==307:\n",
    "        l3.append(0)\n",
    "    elif df['Breed1'][i]!=307 and df['Breed2'][i]==0:\n",
    "        l3.append(1)\n",
    "    elif df['Breed1'][i]==0 and df['Breed2'][i]!=307:\n",
    "        l3.append(1)\n",
    "    elif df['Breed1'][i]==df['Breed2'][i]:\n",
    "        l3.append(1)\n",
    "    else:\n",
    "        l3.append(0)\n",
    "        \n",
    "df_processed2.drop(\"Breed1\",axis=1,inplace=True)\n",
    "df_processed2.drop(\"Breed2\",axis=1,inplace=True) \n",
    "df_processed2.insert(3,'PureBreed',l3) ##INSERT IN DATAFRAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_distr_pairwise('AdoptionSpeed','PureBreed', df_processed2, frel=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the plot and relative gain table we conclude that pure breed animals (1) tend to be adopted earlier, whereas mixed breed animals (0) are adopted later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantity\n",
    "\n",
    "Number of pets represented in profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax3 = sns.catplot(x='Quantity', data=df, kind='count')\n",
    "(ax3.set_axis_labels(\"Quantity\", \"Number of Pets\"))\n",
    "print('There are {} profiles with a single pet'.format((list(df['Quantity'])).count(1)))\n",
    "print('There are {} profiles with multiple pets'.format(len(df['Quantity'])-list(list(df['Quantity'])).count(1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ste = cat_distr_pairwise('AdoptionSpeed','Quantity', df, frel=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"inf\" values on the table indicate that there are no examples of a pet with `AdoptionSpeed = 0` for that feature instance. Therefore, all positive relative gains are 'infinite'.\n",
    "\n",
    "We can conclude that adds containing more than 5 pets could be more likely to be ignored, since those pets are adopted after a long time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fee\n",
    "\n",
    "After observing the fees' distribution we considered 4 categories: \n",
    "* Free => 0\n",
    "* Tens => 1\n",
    "* Hundreds => 2\n",
    "* Thousands => 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f=[]\n",
    "for i in df[\"Fee\"]:\n",
    "    if i==0:\n",
    "        f.append(0)\n",
    "    elif i<100:\n",
    "        f.append(1)\n",
    "    elif i<1000:\n",
    "        f.append(2)\n",
    "    else:\n",
    "        f.append(3)\n",
    "\n",
    "df_processed2.drop(\"Fee\",axis=1,inplace=True) \n",
    "df_processed2.insert(4,'Free',f) ##INSERT IN DATAFRAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fee = cat_distr_pairwise('AdoptionSpeed','Free', df_processed2, frel=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that pets with fees in the order of the thousands are less likely to be adopted fast."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RescuerID\n",
    "\n",
    "In order to extract some meaning from the `RescuerID`, we decided to create a new feature `FrequentRescuer` which indicates whether it is frequent for rescuers to save and advertise pets. After visiting *www.PetFinder.my*, we noticed pet profiles include links to other profiles by the same rescuer. In this way, it is plausible that users, while browsing the website, will look at related profiles more often than profiles by different users, thereby influencing adoption rates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = list(df['RescuerID'].unique())\n",
    "y = list(df['RescuerID'])\n",
    "z = []\n",
    "t = {}\n",
    "for i in x:\n",
    "    z.append(y.count(i))\n",
    "\n",
    "t['Rescuer'] = x\n",
    "t['entries'] = z\n",
    "\n",
    "e = pd.DataFrame(t)\n",
    "print('There are {} total rescuers in the dataset'.format(len(x)))\n",
    "print('There are {} rescuers with only 1 webpage entry in the dataset'.format(z.count(1)))\n",
    "print('There are {} rescuers with more than 1 webpage entry in the dataset'.format(len(x)-(z.count(1))))\n",
    "ax11 = sns.catplot(x='entries', data=e, kind='count', height=5, aspect=3)\n",
    "(ax11.set_axis_labels(\"Web page entries\", \"Number of Rescuers\")\n",
    "    .set_titles(\"{col_name} {col_var}\")\n",
    "    .despine(left=True))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the dataset, the number of rescuers with a single entry in the PetFinder webpage is greater than the number of rescuers who save and advertise more than 1 pet. This information could be useful on later anaylsis to discover whether the number of webpage profile entries associated with a rescuer has any effect in pet adoption/adoption speed.\n",
    "\n",
    "An important aspect of this analysis is that, given we have access to around ~10% of the global dataset, it is possible that many ID's described as non-frequent may be mislabeled (if other instances of the ID are present in the remaining ~90% of data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CREATE NEW FEATURE FREQUENTRESCUER\n",
    "\n",
    "lista2 = []\n",
    "y = list(df['RescuerID'])\n",
    "for i in df['RescuerID']:\n",
    "      if y.count(i)>1:\n",
    "          lista2.append(float(1))\n",
    "      else:\n",
    "          lista2.append(float(0))\n",
    "          \n",
    "df_processed1.insert((df.columns.get_loc(\"RescuerID\"))+1,'FrequentRescuer',lista2) ##INSERT IN DATAFRAME\n",
    "df_processed1.drop(\"RescuerID\", axis=1, inplace=True)\n",
    "\n",
    "df_processed2.insert((df.columns.get_loc(\"RescuerID\"))+1,'FrequentRescuer',lista2) ##INSERT IN DATAFRAME\n",
    "df_processed2.drop(\"RescuerID\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VideoAmt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax2 = sns.catplot(x='VideoAmt', data=df, kind='count')\n",
    "(ax2.set_axis_labels(\"Video Amount\", \"Number of Pets\"))\n",
    "print('There are {} profiles with 0 videos'.format((list(df['VideoAmt'])).count(0)))\n",
    "print('There are {} profiles with more than 0 videos'.format(len(df['VideoAmt'])-list(list(df['VideoAmt'])).count(0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the distribution of pets, we will consider this feature as binary: has Video (1) or does not have Video (0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hasVideo=[]\n",
    "\n",
    "for v in df['VideoAmt']:\n",
    "    if v==0:\n",
    "        hasVideo.append(0)\n",
    "    else:\n",
    "        hasVideo.append(1)\n",
    "        \n",
    "df_processed2.drop(\"VideoAmt\", axis=1,inplace=True)\n",
    "df_processed2.insert(5,\"hasVideo\",hasVideo) ##INSERT IN DATAFRAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Description\n",
    "\n",
    "In order to extract some meaning from the `Description`, we decided, as a first approach, to partition this continuous feature into discrete values.\n",
    "\n",
    "Since empty descriptions (`size=0`) probably have a direct influence in the adoption choice, it should be a category of its own. For that reason, we exclude empty descriptions from the discretization process.\n",
    "\n",
    "We consider that the discretization of the 'description sizes' based on the quantiles strategy is appropriate, since it is plausible to say that the ad writers define what a 'medium'-sized description is. In that sense, a 'medium'-sized description would have the average word count. Analogously, the smallest and largest descriptions would correspond to the first and third quantiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from sklearn import preprocessing\n",
    "import statistics \n",
    "\n",
    "l=[]\n",
    "\n",
    "# List descriptions' lengths (i.e. number of char)\n",
    "for i in df['Description']:\n",
    "    if type(i)!=str: # Some empty descriptions are type:float\n",
    "        l.append(0)\n",
    "    else:\n",
    "        l.append(len(i.split()))\n",
    "\n",
    "mean = statistics.mean(l)\n",
    "print(\"The average descripton size:\" , mean)\n",
    "\n",
    "# Histogram\n",
    "a1 = np.asarray(l)\n",
    "nr_bins = 1 + 3.322*math.log(len(l),2) # Number of bins according to Sturges rule\n",
    "a1 = pd.Series(a1)\n",
    "a1.plot.hist(grid=True, bins = round(nr_bins), rwidth=0.9, color='#607c8e')\n",
    "plt.title('Description size distribution')\n",
    "plt.xlabel('Description size (# words)')\n",
    "plt.ylabel('Counts')\n",
    "plt.grid(axis='y', alpha=0.75)\n",
    "plt.xlim([0.0,1300])\n",
    "\n",
    "# Discretization by quantile strategy:\n",
    "a2 = np.asarray(l).reshape(-1,1) # Reshape data since we are working with just one feature\n",
    "est = preprocessing.KBinsDiscretizer(n_bins=[3],encode='ordinal', strategy='quantile').fit(a2)\n",
    "decp = est.transform(a2)\n",
    "\n",
    "print(\"According to this discretization, the bin edges should be:\")\n",
    "print(est.bin_edges_) # how the data is distributed in the four bins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on these results, we present a new feature 'DescriptionSize' whose possible values include:\n",
    "* `Empty` for descriptions containing 0 words => 0\n",
    "* `Small` for descriptions containing 1-28 words => 1\n",
    "* `Medium` for descriptions containing 29-65 words => 2\n",
    "* `Large` for descriptions containing 66-1257 words => 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create new feature 'Description size'\n",
    "d = []\n",
    "\n",
    "for i in df['Description']:\n",
    "    if type(i)!=str: # Some empty descriptions are type:float\n",
    "        d.append(0)\n",
    "    else:\n",
    "        a= len(i.split())\n",
    "        if a<29:\n",
    "            d.append(1)\n",
    "        elif a<66:\n",
    "            d.append(2)\n",
    "        else:\n",
    "            d.append(3)    \n",
    "            \n",
    "df_processed1.drop(\"Description\", axis=1, inplace=True)\n",
    "df_processed1.insert(6,\"DescriptionSize\",l) ##INSERT IN DATAFRAME\n",
    "\n",
    "df_processed2.drop(\"Description\", axis=1, inplace=True)\n",
    "df_processed2.insert(6,\"DescriptionSize\",d) ##INSERT IN DATAFRAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other conclusions\n",
    "* Pets with maturity size `Small` (1) and `Extra Large`(4) are less likely to be in the adoption centre for a very long time (-45.0552% and -97.3772 of pets at `AdoptionSpeed = 4` relatively to `AdoptionSpeed = 1`, respectively). \n",
    "* Pets with long fur (3) very rarely stay at the adoption centre for a long time (-116.814% of pets at `AdoptionSpeed = 4` relatively to `AdoptionSpeed = 1`).\n",
    "* Sterilized pets tend to be adopted later (+82.13% of pets at `AdoptionSpeed = 4` relatively to `AdoptionSpeed = 1`).\n",
    "* Pets with serious injuries (`Health = 3`) are very more likely to be adopted after a long time, or not adopted at all (+182.091% of pets at `AdoptionSpeed = 4` relatively to `AdoptionSpeed = 1`). \n",
    "* No pets were adopted in the same day having the add not provide a description!\n",
    "* There was no clear evidence that the distribution of the features values among `AdoptionSpeed` classes was biased for features `Gender`, `Vaccinated`, `Dewormed`, `State`, `FrequentRescuer`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1 (Supervised Learning) - Predicting Adoption and Adoption Speed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. Preprocessing Data for Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During our EDA we already preprocessed some of the features, for data visualization purposes. The result are two datasets:\n",
    "* **df_processed1**: which we will use to benchmark the models' performance without the discretization of features discussed earlier, since they can remove the fine grain information that might be beneficial to the learning proccess. Some data cleaning was done, namely in features `PetID`, `Name`, `RescuerID` and `DescriptionSize` (in words, instead of text `Description`).\n",
    "\n",
    "* **df_processed2**: which is the data set with the derived features and data cleaning. We will see if the dicretization will be helpful for the models or not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features impact\n",
    "\n",
    "Now that we have preprocessed our features, one way to determine the impact of each one on prediction models is to apply a random forest. Based on that information, we will  discard some features that have low impact and are probably causing noise.\n",
    "\n",
    "Bellow we have a function that will apply a random forest model to any data set.\n",
    "Code obtained from: <https://towardsdatascience.com/running-random-forests-inspect-the-feature-importances-with-this-code-2b00dd72b92e>\n",
    "\n",
    "Random forest algorithms seek to maximise impurity decreases from node to node in each tree. Features are considered more important the more they decrease impurity in datasets amongst the entire random tree population that was generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_forest(df_x, df_y, test_size):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(df_x, df_y, test_size=test_size, random_state=12)\n",
    "    rf = RandomForestClassifier() \n",
    "    rf.fit(X_train, y_train) \n",
    "    score = rf.score(X_test, y_test)\n",
    "    feature_importance = pd.DataFrame(rf.feature_importances_, index = df_x.columns, columns=['importance']).sort_values('importance',ascending=False).reset_index().rename(columns = {'index':'column'})\n",
    "    return score, feature_importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Original features importance (df_processed1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "score1, feature_importances1 = random_forest(\n",
    "    df_processed1[df_processed1.columns.difference(['AdoptionSpeed'])],\n",
    "    df_processed1['AdoptionSpeed'],\n",
    "    0.2)\n",
    "print(\"Score: \",score1)\n",
    "print(feature_importances1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ADICIONAR COMENTARIO SOBRE EDA VS FEATURE IMPORTANCE, EX.: AGE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extracted features importance (df_processed2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "score2, feature_importances2 = random_forest(\n",
    "    df_processed2[df_processed2.columns.difference(['AdoptionSpeed'])], \n",
    "    df_processed2['AdoptionSpeed'],\n",
    "    0.2)\n",
    "\n",
    "print(\"Score: \",score2) \n",
    "print(feature_importances2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We conclude that, after our feature extractions, some features are actually contributing less to the model then they were before:\n",
    "* `DescriptionSize` in bins (`DescriptionSize` in number of words)\n",
    "* `AgeGroup` (`Age`)\n",
    "* `PureBreed` (`Breed1`, `Breed2`)\n",
    "\n",
    "We will go back to the original features in the classifiers analysis.\n",
    "Some are doing slightly better/did not get worse so we will keep the changes:\n",
    "* `hasVideo` (`VideoAmt`)\n",
    "* `Free` (`Fee`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#APPLY CHANGES TO DATASET - REVERT TO ORIGINAL FEATURES\n",
    "#Description\n",
    "df_processed2.drop(\"DescriptionSize\", axis=1, inplace=True)\n",
    "df_processed2.insert(6,\"DescriptionSize\",df_processed1['DescriptionSize'])\n",
    "\n",
    "#AgeGroup\n",
    "df_processed2.drop(\"AgeGroup\", axis=1, inplace = True)\n",
    "df_processed2.insert(2,\"Age\",df_processed1['Age'])\n",
    "\n",
    "#PureBreed\n",
    "df_processed2.drop(\"PureBreed\", axis=1, inplace=True)\n",
    "df_processed2.insert(6,\"Breed1\",df_processed1['Breed1'])\n",
    "df_processed2.insert(6,\"Breed2\",df_processed1['Breed2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_processed2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data imbalance\n",
    "\n",
    "As we explained in the beggining there is a clear imbalance in the number of examples:\n",
    "- Binary classification: the number of examples of `AdoptionSpeed < 4` (*Adopted*) is much greater than the number of examples for `AdoptionSpeed = 4` (*Not Adopted*).\n",
    "- Multiclass classification: the number of examples of `AdoptionSpeed = 0` is much smaller than the rest of the target classes.\n",
    "\n",
    "Since we are working with such a small dataset, we cannot spare any examples, and we will resort to random oversampling to balance our data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Binary Classification - Random Oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ADAPT TARGET COLUMN FOR BINARY CLASSIFICATION\n",
    "df_processed2_b = df_processed2.copy()\n",
    "df_processed2_b = df_processed2_b.rename(columns={'AdoptionSpeed': 'Adoption'})\n",
    "df_processed2_b = df_processed2_b.replace({'Adoption':[0,2,3]},1)\n",
    "df_processed2_b = df_processed2_b.replace({'Adoption':4},0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RANDOM OVERSAMPLING\n",
    "ros_b = RandomOverSampler(sampling_strategy='minority',random_state=42) #create oversampling class\n",
    "X_ros_b, y_ros_b = ros_b.fit_resample(df_processed2_b.iloc[:,:-1], df_processed2_b['Adoption']) #resample the oversampled dataset\n",
    "\n",
    "# y_ros_b is a np array in some versions and a pd dataframe/series in others \n",
    "# An object-type check must be applied in order to further proccess the data\n",
    "\n",
    "if ( isinstance(y_ros_b, pd.DataFrame) or isinstance(y_ros_b, pd.Series) ):\n",
    "    df_processed2_b_balanced = X_ros_b.join(y_ros_b)\n",
    "    \n",
    "else:\n",
    "    y_ros_b.shape = (21592,1) #reshape array for concatenation\n",
    "    np_processed2_b_balanced = np.concatenate((X_ros_b, y_ros_b), axis=1) #save new data\n",
    "    df_processed2_b_balanced = pd.DataFrame(data=np_processed2_b_balanced, columns=df_processed2_b.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.catplot(x=\"Adoption\",data=df_processed2_b_balanced, kind='count')\n",
    "(ax.set_axis_labels(\"Adoption\", \"Number of Pets\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the oversampling was successful because we now have the same number of examples for adopted and not adopted pets, with a total of 21592 examples.\n",
    "\n",
    "We will be using `df_processed2_b_balanced` as the **balanced, binary task general dataset**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_processed2_b_balanced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multiclass classification - Random Over-Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the oversampling was successful because we now have the same number of examples for all `AdoptionSpeed`, with a total of 20985 examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RANDOM OVERSAMPLING\n",
    "df_processed2_m = df_processed2.copy()\n",
    "ros_m = RandomOverSampler(random_state=42)\n",
    "X_ros_m, y_ros_m = ros_m.fit_resample(df_processed2_m.iloc[:,:-1], df_processed2_m['AdoptionSpeed'])\n",
    "\n",
    "# y_ros_b is a np array in some versions and a pd dataframe in others. \n",
    "# An object-type check must be applied in order to further proccess the data\n",
    "\n",
    "if ( isinstance(y_ros_b, pd.DataFrame) or isinstance(y_ros_b, pd.Series) ):\n",
    "    df_processed2_m_balanced = X_ros_m.join(y_ros_m)\n",
    "    \n",
    "else:\n",
    "    y_ros_m.reshape = (20985,1) #reshape array for concatenation\n",
    "    np_processed2_m_balanced = np.concatenate((X_ros_m, y_ros_m), axis=1) #save new data\n",
    "    df_processed2_m_balanced = pd.DataFrame(data=np_processed2_m_balanced, columns=df_processed2_m.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.catplot(x=\"AdoptionSpeed\",data=df_processed2_m_balanced, kind='count')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Dogs and Cats - preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DOGS BINARY CLASSIFICATION\n",
    "df_processed2_b_dogs = df_processed2_b_balanced.copy()\n",
    "df_processed2_b_dogs = df_processed2_b_balanced[df_processed2_b_dogs['Type']==1]\n",
    "\n",
    "# DOGS MULTICLASS CLASSIFICATION\n",
    "df_processed2_m_dogs = df_processed2_m_balanced.copy()\n",
    "df_processed2_m_dogs = df_processed2_m_dogs[df_processed2_m_dogs['Type']==1]\n",
    "\n",
    "# CATS BINARY CLASSIFICATION\n",
    "df_processed2_b_cats = df_processed2_b_balanced.copy()\n",
    "df_processed2_b_cats = df_processed2_b_cats[df_processed2_b_cats['Type']==2]\n",
    "\n",
    "# CATS MULTICLASS CLASSIFICATION\n",
    "df_processed2_m_cats = df_processed2_m_balanced.copy()\n",
    "df_processed2_m_cats = df_processed2_m_cats[df_processed2_m_cats['Type']==2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data encoding is a necessary step for categorical features. Our machine learning algorithms must receive numerical inputs, and as such, data organised in categories must be converted appropriately. We observe two distinct types of categorical features in our dataset:\n",
    "\n",
    "1) ordinal categorical data, that is, data organised in classes which have a natural order to them;\n",
    "\n",
    "2) nominal categorical data, that is, data organised in classes which are distinct and unordered.\n",
    "\n",
    "Of the first kind, we observe features such as `MaturitySize`, `FurLength` and `Health`. These benefit from label encoding, also known as ordinal encoding, which converts each class to a numerical value that preserves the order in the category. However, the existence of a default value `0` for unspecified fields makes analysis of these features easier if considering them nominal data.\n",
    "\n",
    "The remaining categorical features fit the second type. For these, one hot encoding is a good option. With this method, each class in the category becomes its own binary feature (it either exists or it doesn't in each data entry), therefore eliminating any underlying assumptions of order or distance between classes. This method tends to lose usefulness in cases of high cardinality features; tree-based models are more prone to overfit, as nodes will split in many possible values for each class in each category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function allows the encoding of the features of a given dataset\n",
    "# Inputs: \n",
    "  # df: the dataframe we want to enconde\n",
    "  # enc_features: list of features we want to encode\n",
    "  # nonenc_features: list of features we don't want to encode\n",
    "# Output: encoded dataset\n",
    "\n",
    "def encode(df, enc_features, nonenc_features, binary):\n",
    "    encoder = OneHotEncoder()\n",
    "    df_encoded = encoder.fit_transform(df[enc_features]).toarray()\n",
    "    column_name = encoder.get_feature_names(enc_features)\n",
    "    df_result =  pd.DataFrame(df_encoded, columns= column_name)\n",
    "    \n",
    "    # INSERT THE PETID AS INDEX\n",
    "    df_result.insert(0,'PetID',df.index.tolist())\n",
    "    df_result.set_index(\"PetID\", inplace=True) # change index to PetId\n",
    "    \n",
    "    # INSERT THE NON ENCODED FEATURES IN THE DATAFRAME AGAIN\n",
    "    count=2\n",
    "    for i in nonenc_features:\n",
    "        df_result.insert(count,i,df[i]) \n",
    "        count+=1\n",
    "        \n",
    "    # INSERT THE TARGET CLASS\n",
    "    if binary:\n",
    "        df_result.insert((len(df_result.columns)),'Adoption',df['Adoption']) ## Adoption (binary)\n",
    "    else:\n",
    "        df_result.insert((len(df_result.columns)),'AdoptionSpeed',df['AdoptionSpeed']) ## AdoptionSpeed\n",
    "    \n",
    "    return df_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ENCODING FEATURES\n",
    "\n",
    "#df_processed1\n",
    "enc_features1 = ['Type','Breed1','Breed2','Gender','Color1','Color2','Color3','MaturitySize','FurLength','Vaccinated','Dewormed','Sterilized','Health','State']\n",
    "nenc_features1 = ['hasName','Age','DescriptionSize','Quantity','Fee','FrequentRescuer','VideoAmt','PhotoAmt']\n",
    "df_processed2_encoded = encode(df_processed1, enc_features1, nenc_features1, False)\n",
    "\n",
    "#df_processed1_dogs\n",
    "df_processed1_dogs = df_processed1.copy()\n",
    "df_processed1_dogs = df_processed1_dogs[df_processed1_dogs['Type']==1] # Filter by Type = 1\n",
    "enc_features2 = ['Breed1','Breed2','Gender','Color1','Color2','Color3','MaturitySize','FurLength','Vaccinated','Dewormed','Sterilized','Health','State']\n",
    "df_processed1_dogs_encoded = encode(df_processed1, enc_features2, nenc_features1, False)\n",
    "\n",
    "#df_processed1_cats\n",
    "df_processed1_cats = df_processed1.copy()\n",
    "df_processed1_cats = df_processed1_dogs[df_processed1_cats['Type']==2] # Filter by Type = 2\n",
    "df_processed1_cats_encoded = encode(df_processed1, enc_features2, nenc_features1, False)\n",
    "\n",
    "#df_processed2_b_balanced\n",
    "nenc_features2 = ['hasName','Age','DescriptionSize','Quantity','Free','FrequentRescuer','hasVideo','PhotoAmt']\n",
    "df_processed2_b_balanced_encoded = encode(df_processed2_b_balanced, enc_features1, nenc_features2, True)\n",
    "\n",
    "#df_processed2_m_balanced\n",
    "df_processed2_m_balanced_encoded = encode(df_processed2_m_balanced, enc_features1, nenc_features2, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing: final dataset\n",
    "We have the following dataframes to feed the classifiers in the following phase:\n",
    "* Binary task: `df_processed2_b_balanced_encoded`\n",
    "* Multiclass task: `df_processed2_m_balanced_encoded`\n",
    "* Dogs binary task: `df_processed2_b_dogs_encoded`\n",
    "* Dogs multiclass task: `df_processed2_m_dogs_encoded`\n",
    "* Cats binary task: `df_processed2_b_cats_encoded`\n",
    "* Cats multiclass task: `df_processed2_m_cats_encoded`\n",
    "\n",
    "Remember the (non-encoded) features chosen for these datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(df_processed2_b_balanced.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Learning Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creates a framework for printing confusion matrices side by side\n",
    "# X for some informative string\n",
    "# y_train for the training set targets\n",
    "# y_train_predict for the target predictions by the model\n",
    "# y_test for the test set targets \n",
    "# y_test_predict for the test predictions by the model\n",
    "\n",
    "def confusion (X,y_train,y_train_predict,y_test,y_test_predict):\n",
    "    print('{}'.format(X))\n",
    "    \n",
    "    cm_train = confusion_matrix(y_train, y_train_predict)\n",
    "    cm_test = confusion_matrix(y_test, y_test_predict)\n",
    "    \n",
    "    y = np.unique(y_train)\n",
    "    z = np.unique(y_test)\n",
    "    \n",
    "    f, ax = plt.subplots(1,2) # defines two matrix plots, one for the training set and the other for the test set\n",
    "    sns.heatmap(cm_train.T, square=True, annot = True, fmt='d', cbar=False,\n",
    "                xticklabels=(y),\n",
    "                yticklabels=(y),\n",
    "                ax=ax[0]) # first plot\n",
    "    ax[0].set_xlabel('true label')\n",
    "    ax[0].set_ylabel('predicted label')\n",
    "    ax[0].set_title('Confusion Matrix - Training Set')\n",
    "\n",
    "    sns.heatmap(cm_test.T, square=True, annot = True, fmt='d', cbar=False,\n",
    "                xticklabels=(z),\n",
    "                yticklabels=(z),\n",
    "                ax=ax[1]) # second plot\n",
    "    ax[1].set_xlabel('true label')\n",
    "    ax[1].set_ylabel('predicted label')\n",
    "    ax[1].set_title('Confusion Matrix - Test Set')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creates a framework for model fitting, scoring and target predictions\n",
    "# takes xtrain, ytrain, xtest and ytest as arguments\n",
    "# the arguments xtrain, ytrain, xtest and ytest come from splitting the dataset into training and test sets\n",
    "# takes the classifier (model) as argument \n",
    "# prints classification reports for both training and test sets\n",
    "# returns predicted targets for both training and test sets\n",
    "# returns fitting scores (accuracy) for both training and test sets\n",
    "\n",
    "def model (intro,xtrain,ytrain,xtest,ytest,model):\n",
    "    print (\"{}\\n\".format(intro))\n",
    "    md = model.fit(xtrain,ytrain)\n",
    "    print(\"Accuracy on training set:\\n\")\n",
    "    trainscore = md.score(xtrain, ytrain)\n",
    "    print(\"{}\\n\".format(trainscore))\n",
    "    ytrainp = md.predict(xtrain)\n",
    "    print('Classification report (training):')\n",
    "    print(classification_report(ytrain,ytrainp))\n",
    "    print(\"Accuracy on test set:\\n\") \n",
    "    testscore = md.score(xtest, ytest)\n",
    "    print(\"{}\\n\".format(testscore))\n",
    "    ytestp = md.predict(xtest)\n",
    "    print('Classification report (test):')\n",
    "    print(classification_report(ytest,ytestp))\n",
    "    return ytrainp, ytestp, trainscore, testscore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.1 Benchmark\n",
    "Before going into training the classifiers with our altered datasets, we want to perform a benchmark of the original dataset's performance in the models (`df_processed1`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function separates examples from targets and optionally splits it into training and testing\n",
    "def separate_split(df,split):\n",
    "    nc = df.shape[1]\n",
    "    matrix = df.values # Convert dataframe to darray\n",
    "    examples = matrix [:, 0:nc-1] # get features \n",
    "    target = matrix [:, nc-1] # get class (last columns)           \n",
    "    fnames = df.columns.values[0:nc-1] #get features names\n",
    "    tname = df.columns.values[nc-1] #get target name\n",
    "    examples = examples.astype(float)\n",
    "    target = target.astype(float)\n",
    "    if not split:\n",
    "        return examples, target, fnames, tname\n",
    "    else:\n",
    "        X_train, X_test, y_train, y_test = train_test_split(examples, target, random_state=0)\n",
    "        print(\"Number of examples and features for the training dataset: \", X_train.shape)\n",
    "        print(\"Number of examples and features for the test dataset: \", X_test.shape)\n",
    "        return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = separate_split(df_processed1_encoded,True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.1.1 Predicting Adoption (binary classification task)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since this is a binary task, `AdoptionSpeed` has to be converted into a binary target class (`0` for profiles with `AdoptionSpeed = 4` and `1` for profiles with `AdoptionSpeed < 4`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function converts an array of multiclass targets into binary classes\n",
    "def binarize(table_y):\n",
    "    for i in range(len(table_y)):\n",
    "        if table_y[i]<4:\n",
    "            table_y[i]=1\n",
    "        else:\n",
    "            table_y[i]=0\n",
    "    return table_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_binary = binarize(y_train)\n",
    "y_test_binary = binarize(y_test)\n",
    "print(\"Number of positives cases in the training set is: {}\".format(np.count_nonzero(y_train_binary == 1)))\n",
    "print(\"Number of negative cases in the training set is: {}\".format(np.count_nonzero(y_train_binary == 0)))\n",
    "print(\"Ratio of positive to negative cases in the training set: {}\".format((np.count_nonzero(y_train_binary == 1))/(np.count_nonzero(y_train_binary == 0))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Different types of machine learning algorithms will be used to solve the task, as a way to see which will be the most appropriate. Model hyperparameters' choice can be determined manually or via automatic methods, such as grid search.\n",
    "\n",
    "In order to optimise the model, the grid search method does an exhaustive assessment of best hyperparameters for the estimator. The grid search will go through all possible hyperparameter combinations and evaluate each model using a few metrics for scoring.\n",
    "\n",
    "5-fold cross-validation is the default model evaluation in this case. The training data is split in 5 equally-sized subsets (hence, the \"5-fold\"), and the model will be trained 5 times, each time leaving a different subset out, to be a test set. The performance of the model is the average of the performance over all (5) different test sets. Below, there is an image explaining the process.\n",
    "\n",
    "<img src=https://scikit-learn.org/stable/_images/grid_search_cross_validation.png width=\"500\">\n",
    "\n",
    "Classification reports can be broken down in 4 columns of precision, recall, score and support:\n",
    "\n",
    "* 1 row for each target class;\n",
    "* 1 row for the overall accuracy.\n",
    "\n",
    "Since the scoring is F1 macro:\n",
    "* 1 row for macro averages;\n",
    "* 1 row for weighted averages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Distance-based Models - K-Nearest Neighbours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_bench_binary = neighbors.KNeighborsClassifier(n_neighbors=3) # 3 neighbors for starters\n",
    "knn_bench_binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining and applying a grid search cross validation to find the best hyperparameters for the knn estimator \n",
    "# adequate metric, adequate p for the minkowski metric, adequate number of neighbours and weights\n",
    "\n",
    "knn_bench_binary_p = {'metric': ['minkowski','chebyshev'],\n",
    "                      'p': [1,2,3,4,5],\n",
    "                      'n_neighbors': [2,4,6,8,10,20], # odd numbers were also tested, ommitted for running time sake\n",
    "                      # 'n_neighbors': [2,3,4,5,6,7], # alternative with even and odd numbers\n",
    "                      'weights': ['uniform','distance']}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 2 * 5 * 6 * 2 = 120 different combinations of hyperparameters, meaning that the grid search will create all these different versions of the model, which are then fit to the training data. Since each model goes through 5-fold cross-validation, there are 120 * 5 = 600 fits in total."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The scoring metric for the grid searh in the benchmark task will be the score of the estimator (runs slow)\n",
    "knn_bench_binary_grid = GridSearchCV(knn_bench, knn_bench_p, scoring='f1_macro',verbose=10) # creates all 120 versions of the KNN model\n",
    "knn_bench_binary_grid.fit(X_train,y_train_binary) # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_bench_binary_grid.best_estimator_ # finding the best hyperparameters from the grid search\n",
    "knn_bench_binary = knn_bench_binary_grid.best_estimator_ # assigning the parameters to the KNN estimator\n",
    "knn_bench_binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred_bench_binary_knn, y_test_pred_bench_binary_knn, trainscore_bench_binary_knn, testscore_bench_binary_knn = model('BENCHMARK - K-Nearest Neighbours: Binary Prediction of Adoption',X_train,y_train_binary,X_test,y_test_binary,knn_bench_binary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion ('BENCHMARK - K-Nearest Neighbours: Binary Prediction of Adoption',y_train_binary,y_train_pred_bench_binary_knn,y_test_binary,y_test_pred_bench_binary_knn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Probabilistic Models - Naïve Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Naïve Bayes algorithm has many different variations, such as:\n",
    "\n",
    "1) *Gaussian*, best used in continuous numerical datasets (presumes a normal distribution of values);\n",
    "    \n",
    "2) *Multinomial*, when features represent frequency counts/number of occurrences;\n",
    "    \n",
    "3) *Bernoulli*, for datasets with binary categories;\n",
    "    \n",
    "4) *Complement*, which is designed for imbalanced datasets.\n",
    "\n",
    "An important aspect to consider in Naïve Bayes models is the underlying assumption that all features are independent from each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bnb_bench = BernoulliNB()\n",
    "bnb_bench"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred_bench_BNB, y_test_pred_bench_BNB, trainscore_bench_BNB, testscore_bench_BNB = model('BENCHMARK - Bernoulli Naïve Bayes: Binary Prediction of Adoption',X_train,y_train_binary,X_test,y_test_binary,bnb_bench)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion ('BENCHMARK - Bernoulli Naïve Bayes: Binary Prediction of Adoption',y_train_binary,y_train_pred_bench_BNB,y_test_binary,y_test_pred_bench_BNB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Tree Models - Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dec_trees_b = tree.DecisionTreeClassifier() # criterion = \"Gini\"\n",
    "\n",
    "dec_trees_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred_b_trees, y_test_pred_b_trees, trainscore_b_trees, testscore_b_trees = model('DECISION TREES',X_train,y_train_binary,X_test,y_test_binary,dec_trees_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion ('Dec Trees',y_train_binary,y_train_pred_b_trees,y_test_binary,y_test_pred_b_trees)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Linear Models - Support Vector Machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsvm_b = LinearSVC(max_iter=15000) # max_iter=1000 (default) ## Didnt converge\n",
    "lsvm_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred_b_svm, y_test_pred_b_svm, trainscore_b_svm, testscore_b_svm = model('Support Vector Machine',X_train,y_train_binary,X_test,y_test_binary,lsvm_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion ('SVC',y_train_binary,y_train_pred_b_svm,y_test_binary,y_test_pred_b_svm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Linear Models - Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg_b = LogisticRegression(solver='lbfgs', max_iter=4000)\n",
    "logreg_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred_b_logr, y_test_pred_b_logr, trainscore_b_logr, testscore_b_logr = model('Logistic Regression',X_train,y_train_binary,X_test,y_test_binary,logreg_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion ('LogR',y_train_binary,y_train_pred_b_logr,y_test_binary,y_test_pred_b_logr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given these results we conclude that:\n",
    "* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.1.2 Predicting AdoptionSpeed (Multiclass classification task)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_m = neighbors.KNeighborsClassifier()\n",
    "knn_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining and applying a grid search cross validation to find the best hyperparameters for the knn estimator \n",
    "# adequate metric, adequate p for the minkowski metric, adequate number of neighbours and weights\n",
    "knn_m_p ={'metric':['minkowski','chebyshev'],\n",
    "          'p':[1,2,3,4,5],\n",
    "          'n_neighbors':[2,4,6,8,10,20],\n",
    "          'weights':['uniform','distance']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The scoring metric for the grid searh in the benchmark task will be the score of the estimator (runs slow)\n",
    "knn_m_grid = GridSearchCV(knn_m, knn_m_p, scoring='f1_macro',verbose=10) #used f1 score as a scoring metric for grid searc cv\n",
    "knn_m_grid.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_m_grid.best_estimator_ #finding the best hyperparameters from the grid seach\n",
    "knn_m = knn_m_grid.best_estimator_ #assigning the parameters to the knn estimator\n",
    "knn_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred_m_KNN, y_test_pred_m_KNN, trainscore_m_knn, testscore_m_knn = model('KNN',X_train,y_train,X_test,y_test,knn_m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion ('KNN',y_train,y_train_pred_m_KNN,y_test,y_test_pred_m_KNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* From the test set confusion matrix we understand the model clearly overfitted on the training data with perfect accuracy.\n",
    "* The model didn't perform that well on the test set mainly due to the precision and recall performance of the adoption speed=0 class which lowers the overall performance of this metric.\n",
    "\n",
    "This results are based on the unbalanced dataset. Perhaps with the random oversampling the overall performance of this model will increase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PROBABILISTIC MODELS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the task of prediting adoption speed using probabilistic models we considered 4 distinct Naive Bayes approaches:\n",
    "* Gaussian Naive Bayes\n",
    "* Multinomial Naive Bayes\n",
    "* Bernoulli Naive Bayes\n",
    "* Complement Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnb_m = GaussianNB() # gaussian naive bayes\n",
    "mnb_m = MultinomialNB() # multinomial naive bayes\n",
    "bnb_m = BernoulliNB() # bernoulli naive bayes\n",
    "knb_m = ComplementNB() # Complement naive bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Gaussian Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred_m_GBN, y_test_pred_m_GBB, trainscore_m_gnb, testscore_m_gnb = model('Gaussian Naive Bayes',X_train,y_train,X_test,y_test,gnb_m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion ('GaussianNaiveBayes',y_train,y_train_pred_m_GBN,y_test,y_test_pred_m_GBB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The gaussian bayes classifier performed decently both on training and test sets on identifying the elements of the minority class with a almost perfect recall. However, the model seems to have failed in identifying the characteristics that separate the minority class from the other classes as expressed in a low precision value.\n",
    "\n",
    "* For the remaining classes the precision is better but the model fails to identify all the relevant samples from each category (low recall)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred_m_MNB, y_test_pred_m_MNB, trainscore_m_MNB, testscore_m_MNB = model('Multinomial Naive Bayes',X_train,y_train,X_test,y_test,mnb_m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion ('MultinomialNaiveBayes',y_train,y_train_pred_m_MNB,y_test,y_test_pred_m_MNB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Bernoulli Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred_m_BNB, y_test_pred_m_BNB, trainscore_m_BNB, testscore_m_BNB = model('Bernoulli Naive Bayes',X_train,y_train,X_test,y_test,bnb_m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion ('BernoulliNaiveBayes',y_train,y_train_pred_m_BNB,y_test,y_test_pred_m_BNB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Complement Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred_m_KNB, y_test_pred_m_KNB, trainscore_m_KNB, testscore_m_KNB = model('Complement Naive Bayes',X_train,y_train,X_test,y_test,knb_m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion ('ComplementNaiveBayes',y_train,y_train_pred_m_KNB,y_test,y_test_pred_m_KNB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* All models besides the Gaussian Naive Bayes present similar results in regards to the overall performance of the model.\n",
    "* All models have a farly similiar performance on both training and test sets which might hint underfitting.\n",
    "* Amongst the three remaining models, Bernoulli Naive Bayes shows a better overall compromise between precision and recall, although it fails to \"understand\" the minority class.\n",
    "* Oversampling and perhaps feature normalization would help improving the models values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DECISION TREES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dec_trees_m = tree.DecisionTreeClassifier()\n",
    "dec_trees_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining and applying a grid search cross validation to find the best hyperparameters for the Decision tree estimator\n",
    "dec_trees_p ={'class_weight':[None,'balanced'],\n",
    "          'criterion':['gini','entropy'],\n",
    "          'max_features':[None,'auto','log2'],\n",
    "          'min_samples_leaf':[1,2,3,4,5,6],\n",
    "          'min_samples_split':[2,3,4,5]}\n",
    "\n",
    "dec_trees_m_grid = GridSearchCV(dec_trees_m, dec_trees_p, scoring='f1_macro',verbose=10)\n",
    "dec_trees_m_grid.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dec_trees_m_grid.best_estimator_\n",
    "dec_trees_m = dec_trees_m_grid.best_estimator_\n",
    "dec_trees_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred_m_trees, y_test_pred_m_trees, trainscore_m_trees, testscore_m_trees = model('DECISION TREES',X_train,y_train,X_test,y_test,dec_trees_m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion ('Decision Trees',y_train,y_train_pred_m_trees,y_test,y_test_pred_m_trees)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The performance on the training set is good which doesn't happen on the test set. This might indicate overfitting.\n",
    "* The performance on the test set is decent being clearly spoiled by the minority class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LINEAR MODELS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the linear model task of predicting pet's adoptions speeds we will test four different approaches:\n",
    "* Linear Support Vector Machines\n",
    "* Logistic Regression\n",
    "* Perceptron\n",
    "* Ridge Classifier\n",
    "\n",
    "For each of this approaches we will use grid search to find the best possible hyperparameters for each model using each estimators score as the performance metric for grid search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining and applying a grid search cross validation to find the best hyperparameters for each linear estimator\n",
    "lsvm_m_p = {'C':[0.001,0.01,0.1,1,10],\n",
    "            'class_weight':[None,'balanced'],\n",
    "            'loss':['hinge','squared_hinge'],\n",
    "            'max_iter':[10000],\n",
    "            'penalty':['l2']}\n",
    "\n",
    "lreg_m_p = {'C':[0.001, 0.01,0.1,1,10],\n",
    "            'class_weight':[None,'balanced'],\n",
    "            'max_iter':[7000],\n",
    "            'multi_class':['auto'],\n",
    "            'penalty':['l2'],\n",
    "            'solver':['lbfgs','newton-cg']}\n",
    "\n",
    "per_m_p = {'alpha':[0.0001,0.001,0.01,0.1,1,10],\n",
    "          'class_weight':[None,'balanced'],\n",
    "          'early_stopping':[True],\n",
    "          'max_iter': [5000],\n",
    "          'n_iter_no_change':[10],\n",
    "          'penalty':[None,'l1','l2']}\n",
    "\n",
    "rdcl_m_p = {'alpha':[0.0001,0.001,0.01,0.1,1,10],\n",
    "          'class_weight':[None,'balanced'],\n",
    "          'max_iter': [5000],\n",
    "          'normalize':[True,False],\n",
    "          'solver':['auto','svd', 'cholesky', 'lsqr', 'sparse_cg']}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Linear support vector machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsvm_m = LinearSVC(max_iter=40000) # linear support vector machine\n",
    "lsvm_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred_m_lsvm, y_test_pred_m_lsvm, trainscore_m_lsvm, testscore_m_lsvm = model('Linear Support Vector Machine',X_train,y_train,X_test,y_test,lsvm_m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion ('Linear Support Vector Machines',y_train,y_train_pred_m_lsvm,y_test,y_test_pred_m_lsvm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear support vector machine failed do converge with as much as 40000 iterations. This might indicate this is not a good model for this task. We won't therefore be using this model for feature optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Logistic regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lreg_m = LogisticRegression() # logistic regression\n",
    "lreg_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lreg_m_grid = GridSearchCV(lreg_m, lreg_m_p, scoring='f1_macro',verbose=10)\n",
    "lreg_m_grid.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lreg_m = lreg_m_grid.best_estimator_\n",
    "lreg_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred_m_lregm, y_test_pred_m_lregm, trainscore_m_lregm, testscore_m_lregm = model('Logistic Regression',X_train,y_train,X_test,y_test,lreg_m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion ('Logistic Regression',y_train,y_train_pred_m_lregm,y_test,y_test_pred_m_lregm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During the iterative process of finding the best hyperparameters some converge warnings were presented hinting the need for dataset scaling. Perhaps this should be a good approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "per_m = Perceptron() # Perceptron\n",
    "per_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "per_m_grid = GridSearchCV(per_m, per_m_p, scoring='f1_macro', verbose=10)\n",
    "per_m_grid.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "per_m = per_m_grid.best_estimator_\n",
    "per_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred_m_perm, y_test_pred_m_perm, trainscore_m_perm, testscore_m_perm = model('Perceptron',X_train,y_train,X_test,y_test,per_m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion ('Perceptron',y_train,y_train_pred_m_perm,y_test,y_test_pred_m_perm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Ridge Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdcl_m =  RidgeClassifier() #Ridge Classifier\n",
    "rdcl_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdcl_m_grid = GridSearchCV(rdcl_m, rdcl_m_p, scoring='f1_macro',verbose=10)\n",
    "rdcl_m_grid.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdcl_m = rdcl_m_grid.best_estimator_\n",
    "rdcl_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred_m_rdclm, y_test_pred_m_rdclm, trainscore_m_rdclm, testscore_m_rdclm = model('Ridge Classifier',X_train,y_train,X_test,y_test,rdcl_m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion ('Ridge Classifier',y_train,y_train_pred_m_rdclm,y_test,y_test_pred_m_rdclm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2.2 Predicting Adoption (binary classification task)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the dataset `df_processed2_b_balanced_encoded` (binary target, balanced examples and encoded features), we will run the same algorithms as in the previous section, **1.2.1.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the dataset into training and test sets\n",
    "X_train, X_test, y_train_binary, y_test_binary = separate_split(df_processed2_b_balanced_encoded,True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K-Nearest Neighbours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_binary = neighbors.KNeighborsClassifier(n_neighbors=3) # 3 neighbors for starters\n",
    "knn_binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining and applying a grid search cross validation to find the best hyperparameters for the knn estimator \n",
    "# adequate metric, adequate p for the minkowski metric, adequate number of neighbours and weights\n",
    "\n",
    "knn_bench_binary_p = {'metric': ['minkowski','chebyshev'],\n",
    "                      'p': [1,2,3,4,5],\n",
    "                      'n_neighbors': [2,4,6,8,10,20], # odd numbers were also tested, ommitted for running time sake\n",
    "                      # 'n_neighbors': [2,3,4,5,6,7], # alternative with even and odd numbers\n",
    "                      'weights': ['uniform','distance']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The scoring metric for the grid searh in the benchmark task will be the score of the estimator (runs slow)\n",
    "knn_bench_binary_grid = GridSearchCV(knn_bench, knn_bench_p, scoring='f1_macro',verbose=10) # creates all 120 versions of the KNN model\n",
    "knn_bench_binary_grid.fit(X_train,y_train_binary) # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_bench_binary_grid.best_estimator_ # finding the best hyperparameters from the grid search\n",
    "knn_bench_binary = knn_bench_binary_grid.best_estimator_ # assigning the parameters to the KNN estimator\n",
    "knn_bench_binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred_bench_binary_knn, y_test_pred_bench_binary_knn, trainscore_bench_binary_knn, testscore_bench_binary_knn = model('K-Nearest Neighbours: Binary Prediction of Adoption',X_train,y_train_binary,X_test,y_test_binary,knn_bench_binary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion ('K-Nearest Neighbours: Binary Prediction of Adoption',y_train_binary,y_train_pred_bench_binary_knn,y_test_binary,y_test_pred_bench_binary_knn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naïve Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bnb_binary = BernoulliNB()\n",
    "bnb_binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred_binary_BNB, y_test_pred_binary_BNB, trainscore_binary_BNB, testscore_binary_BNB = model('Bernoulli Naïve Bayes: Binary Prediction of Adoption',X_train,y_train_binary,X_test,y_test_binary,bnb_binary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion ('Bernoulli Naïve Bayes: Binary Prediction of Adoption',y_train_binary,y_train_pred_binary_BNB,y_test_binary,y_test_pred_binary_BNB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.3. Predicting pet-specific models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this part we will only test the models that had the best performance on the previous tasks, namely XXXXX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.3.1 DOGS (Type 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Binary Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples, target, fnames, tname = separate_split(df_processed1_dogs_encoded,False)\n",
    "target_b = binarize(target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Multiclass Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples, target, fnames, tname = separate_split(df_processed1_dogs_encoded,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.3.2 CATS (Type 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Binary Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples, target, fnames, tname = separate_split(df_processed1_cats_encoded,False)\n",
    "target_b = binarize(target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Multiclass Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples, target, fnames, tname = separate_split(df_processed1_cats_encoded,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2.3 Predicting Adoption Speed (Multiclass classification task)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2.4 Predicting Adoption/Adoption Speed for dogs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2.5 Predicting Adoption/Adoption Speed for cats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3. Classification - Results and Discussion "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2 (Unsupervised Learning) - Charactering Adopted Pets and Adoption Speed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this task you should **use unsupervised learning algorithms and try to characterize pets that were actually adopted and their adoption speed**. You can use:\n",
    "* **Association rule mining** to find **associations between the features and the target Adoption/AdoptionSpeed**.\n",
    "* **Clustering algorithms to find similar groups of pets**. Is it possible to find groups of pets with the same/similar adoption speed.\n",
    "* **Be creative and define your own unsupervised analysis!** What would it be interesting to find out ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Preprocessing Data for Association Rule Mining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Finding Associations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Association Rules - Results and Discussion "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4. Preprocessing Data for Clustering\n",
    "\n",
    "For this task we will use the same processed dataset as we used for the Binary and Multiclass Classification tasks, df_processed2_b_balanced_encoded and df_processed2_m_balanced_encoded. k-Means uses distance computation as a metric in its algorithm, and hence cannot handle categorical variable directly. That is the reason why we use the encoded datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SPLIT THE DATASET\n",
    "nc_b = df_processed2_b_balanced_encoded.shape[1] # number of columns\n",
    "table_X_b = df_processed2_b_balanced_encoded.iloc[:, 0:nc_b-1] # get features ignoring AdoptionSpeed\n",
    "table_y_b = df_processed2_b_balanced_encoded.iloc[:, nc_b-1] # get class (last columns) \n",
    "\n",
    "nc_m = df_processed2_m_balanced_encoded.shape[1] # number of columns\n",
    "table_X_m = df_processed2_m_balanced_encoded.iloc[:, 0:nc_m-1] # get features ignoring AdoptionSpeed\n",
    "table_y_m = df_processed2_m_balanced_encoded.iloc[:, nc_m-1] # get class (last columns) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each dimension has the same weight in the k-means algorithm, but since our data has dimensions in different scales (some are binary, ranging from 1 to 0 while others are continuous, ranging from 0 to 250 - `Age`) we will proceed to standardizing the data, in order to avoid distortion on the relative near-ness of observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STANDARDIZING THE DATA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "data_scaled_b = scaler.fit_transform(table_X_b)\n",
    "data_scaled_m = scaler.fit_transform(table_X_m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5. Finding Groups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to find out if it's possible to find groups of pets with the same/similar adoption speed, we will look at the k=2 and k=5 clusters obtained from k-Means and determine whether they coincide with the AdoptionSpeed groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "kmeans_b = KMeans(n_clusters=2, random_state=0)\n",
    "kmeans_m = KMeans(n_clusters=5, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train K-means\n",
    "kmeans_b = kmeans_b.fit(data_scaled_b)\n",
    "kmeans_m = kmeans_m.fit(data_scaled_m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function produces a confusion matrix with the percentual membership\n",
    "# (of the points in each cluster) to a given feature\n",
    "\n",
    "def df_confusion(cluster, table_a):\n",
    "    df = pd.DataFrame(set(table_a))\n",
    "    df = df.set_index(0)\n",
    "\n",
    "    for i in set(cluster): df[i] = 0 \n",
    "    i=0\n",
    "    while i < len(cluster):\n",
    "        df.loc[table_a[i], cluster[i]]+=1\n",
    "        i += 1\n",
    "    for i in set(df.columns): \n",
    "        Total = df[i].sum()\n",
    "        for j in set(df.index):\n",
    "            df.loc[j, i] = (df.loc[j, i] / Total)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5.1 Binary analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare membership for binary dataset\n",
    "df1 = df_confusion(kmeans_b.labels_, table_y_b)\n",
    "ax = sns.heatmap(df1, annot=True)\n",
    "ax.set_xlabel('kmeans label')\n",
    "ax.set_ylabel('true label')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5.1 Multiclass analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare membership for multiclass dataset\n",
    "df2 = df_confusion(kmeans_m.labels_, table_y_m)\n",
    "ax = sns.heatmap(df2, annot=True)\n",
    "ax.set_xlabel('kmeans label')\n",
    "ax.set_ylabel('true label')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.6. Clustering - Results and Discussion "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In respect to the Binary analysis, we can see in the confusion matrix that the negative examples are evenly distributed through both clusters obtained from k-means, and the same happens for the positive examples. Thus, we conclude that the groups obtained from k-means do not coincide with the groups from the target class. \n",
    "\n",
    "In the Multiclass analysis, the confusion matrix indicates that none of the clusters hold more than 30% of examples from any AdoptionSpeed class, thus we conclude that there is no evident agreement between the predicted clusters and the real target groups.\n",
    "\n",
    "These poor results can be explained due to the complexity and high dimensionality of the dataset, because when we have so many features (378) distance interpretation isn’t so obvious. An additional step employing Principal Component Analysis to reduce dimensionality might've helped get more productive results. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Final Comments and Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
